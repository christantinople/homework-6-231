---
title: "Homework 6"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Tree-Based Models

For this assignment, we will continue working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Houndoom, a Dark/Fire-type canine Pokémon from Generation II.](images/houndoom.jpg){width="200"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics.

**Note: Fitting ensemble tree-based models can take a little while to run. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit.**

```{r}
# packages
library(tidyverse)
library(tidymodels)
library(ISLR)
library(rpart.plot)
library(vip)
library(janitor)
library(randomForest)
library(ranger)
#library(xgboost)
library(corrplot)
```

### Exercise 1

Read in the data and set things up as in Homework 5:

- Use `clean_names()`
- Filter out the rarer Pokémon types
- Convert `type_1` and `legendary` to factors

```{r}
# read in data
pokemon <- read.csv("data/Pokemon.csv")

# clean up column names
pokemon <- clean_names(pokemon)

# filter the pokemon types to make life easier
mons <- pokemon%>%
  filter(
      type_1=="Bug"|
      type_1=="Fire"|
      type_1=="Grass"|
      type_1=="Normal"|
      type_1=="Water"|
      type_1=="Psychic"
    )

# convert `type_1` and `legendary` to a factor
mons$type_1 <- as.factor(mons$type_1)
mons$legendary <- as.factor(mons$legendary)
```

Do an initial split of the data; you can choose the percentage for splitting. Stratify on the outcome variable.

```{r}
set.seed(42069) # for reproducibiility

# get training and testing data
pokemon_split <- initial_split(mons,prop=0.8,
                               strata=type_1)

pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)
```

Fold the training set using *v*-fold cross-validation, with `v = 5`. Stratify on the outcome variable.

```{r}
# cross-validation folds
pokemon_folds <- vfold_cv(pokemon_train, v=5, strata=type_1)
```

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`:

- Dummy-code `legendary` and `generation`;
- Center and scale all predictors.

```{r}
# create recipe to predict `type_1`
pokemon_recipe <- recipe(type_1 ~ ., data = mons%>%dplyr::select(type_1,hp:legendary))%>%
  step_dummy(c("legendary", "generation"))%>%
  step_center(all_predictors())%>%
  step_scale(all_predictors())
```

### Exercise 2

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the continuous variables for this plot; justify your decision(s).*

What relationships, if any, do you notice? Do these relationships make sense to you?

```{r}
corr_mat <- model.matrix(~0+., data=pokemon_train%>%select(-c(x,name,generation,type_2,type_1))) %>% 
  cor(use="pairwise.complete.obs")

corr_mat%>%
  corrplot(method="number")
```
I chose not to represent Pokedex number (`x`), `name`, `generation`, nor the Pokemons' types in the diagram. Pokedex number and Pokemon name clearly aren't relevant in the models we are using. Name would be important if we had some sort of Natural Language Processing element to this, but we don't. There's just too many types and it makes the diagram look bad, and generation is not too relevant either. 

However, Legendary status is definitiely gonna correlate with the bulk of the predictors, being the stats.

All of the stats are positively correlated with each other, but they are not strongly correlated. We can notice that `attack` and `sp_atk` are pretty strongly correlated with `total`. This is probably because `attack` or `sp_atk` will commonly be the highest base stat, meaning it will take up most of the base stat *total*.

### Exercise 3

First, set up a decision tree model and workflow. Tune the `cost_complexity` hyperparameter. Use the same levels we used in Lab 7 -- that is, `range = c(-3, -1)`. Specify that the metric we want to optimize is `roc_auc`. 

Print an `autoplot()` of the results. What do you observe? Does a single decision tree perform better with a smaller or larger complexity penalty?

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune()))%>%
  add_recipe(pokemon_recipe)
```

```{r}
set.seed(42069)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf, 
  resamples = pokemon_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
```

```{r}
autoplot(tune_res)
```

A single decision tree seems to perform better with a smaller complexity penalty. Having a more complex tree may be advantageous since we only have one tree.

### Exercise 4

What is the `roc_auc` of your best-performing pruned decision tree on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
class_tree_roc_auc <- collect_metrics(tune_res)%>%
  arrange(desc(mean))
class_tree_roc_auc%>%
  head()
```

We see the the best `roc_auc` metric is 0.6364470	corresponding to a `cost_complexity` of 0.004641589.	

### Exercise 5

Using `rpart.plot`, fit and visualize your best-performing pruned decision tree with the *training* set.

```{r}
best_complexity <- select_best(tune_res)

class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)

class_tree_final_fit <- fit(class_tree_final, data = pokemon_train)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```

### Exercise 5

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`. Tune `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why not. What type of model would `mtry = 8` represent?**

```{r}
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

rf_wf <- workflow() %>%
  add_model(rf_spec)%>%
  add_recipe(pokemon_recipe)

param_grid <- grid_regular(mtry(range = c(1,8)),
                           trees(range = c(2,500)),
                           min_n(range = c(2,40)),
                           levels = 8)
```

`mtry` is an integer which represents the number of predictors used. For example, if `mtry` = 4, then 4 predictors are randomly selected at each split of the tree model. The `trees` argument is the number of trees in the ensemble. The `min_n` is the minimium number of observations that have to trace down the tree and land on a certain node in order for that node to be able to split further. This would help prevent overfitting.

### Exercise 6

Specify `roc_auc` as a metric. Tune the model and print an `autoplot()` of the results. What do you observe? What values of the hyperparameters seem to yield the best performance?

```{r, eval=FALSE}
set.seed(42069)

tune_res <- tune_grid(
  rf_wf, 
  resamples = pokemon_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
```

```{r,include=FALSE}
load("three_param_tune_res.rda")
```

```{r}
autoplot(tune_res)
```
The `trees` parameter seems to be the most important. Performance is basically identitical for most models with more nodes. The performance is slightly better for smaller values of `min_n`, and it seems like `mtry` doesn't affect the performance much at all.

### Exercise 7

What is the `roc_auc` of your best-performing random forest model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*

```{r}
rf_roc_auc <- collect_metrics(tune_res)%>%
  arrange(desc(mean))
rf_roc_auc%>%
  head()
```
We see the the best `roc_auc` metric is 0.7425208	 corresponding to `mtry` = 3, `trees` = 144, and `min_n` = 2.

### Exercise 8

Create a variable importance plot, using `vip()`, with your best-performing random forest model fit on the *training* set.

Which variables were most useful? Which were least useful? Are these results what you expected, or not?

```{r}
best_params <- select_best(tune_res)

rf_final <- finalize_workflow(rf_wf, best_params)

rf_final_fit <- fit(rf_final, data = pokemon_train)

rf_final_fit%>%
  pull_workflow_fit()%>%
  vip()
```
All of the stats were quite comparitively useful. Based on my knowledge of pokemon, this order of the stats is roughly what I would expect. I expected `legendary` to be the least useful since there are so few legendary pokemon compared to non-legendary, and within the legendary pokemon, their types are going to spread out; I intuitively wouldn't think of legendary status as a good predictor for the primary type.

`generation` seems like it could be slightly useful, as in the distributions of primary types of Pokemon are probably slightly different between generations of Pokemon, but this information would not be a very concrete predictor of primary type since all primary types are plentiful among all 6 generations used in our data set (why aren't we using an up-to-date list with the current 8 generations?)

### Exercise 9

Finally, set up a boosted tree model and workflow. Use the `xgboost` engine. Tune `trees`. Create a regular grid with 10 levels; let `trees` range from 10 to 2000. Specify `roc_auc` and again print an `autoplot()` of the results. 

What do you observe?

What is the `roc_auc` of your best-performing boosted tree model on the folds? *Hint: Use `collect_metrics()` and `arrange()`.*
```{r}
boost_spec <- boost_tree(trees = tune()) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

boost_wf <- workflow() %>%
  add_model(boost_spec)%>%
  add_recipe(pokemon_recipe)

trees_grid <- grid_regular(trees(range = c(10,2000)), levels = 10)

set.seed(42069)

tune_res <- tune_grid(
  boost_wf, 
  resamples = pokemon_folds, 
  grid = trees_grid, 
  metrics = metric_set(roc_auc)
)

autoplot(tune_res)
```

We observe that the ROC AUC value steeply increases with the number of trees up to around 500, and then the values kinda oscillate around a constant value as the number of trees increases.

```{r}
boost_roc_auc <- collect_metrics(tune_res)%>%
  arrange(desc(mean))
boost_roc_auc%>%
  head()
```
The best ROC AUC value is 0.7079029	corresponding to 1778 trees.
### Exercise 10

Display a table of the three ROC AUC values for your best-performing pruned tree, random forest, and boosted tree models. Which performed best on the folds? Select the best of the three and use `select_best()`, `finalize_workflow()`, and `fit()` to fit it to the *testing* set. 

Print the AUC value of your best-performing model on the testing set. Print the ROC curves. Finally, create and visualize a confusion matrix heat map.

Which classes was your model most accurate at predicting? Which was it worst at?

```{r}
best_roc_auc <- data.frame(roc_auc = c(class_tree_roc_auc$mean[1],
                                       rf_roc_auc$mean[1],
                                       boost_roc_auc$mean[1]),
                           model = c("Pruned Tree",
                                     "Random Forest",
                                     "Boosted Tree"))
best_roc_auc%>%arrange(desc(roc_auc))
```
```{r}
# get the tune_res from the random forest
load("three_param_tune_res.rda")

best_params <- select_best(tune_res)

rf_final_wf <- finalize_workflow(rf_wf, best_params)

rf_final_fit <- fit(rf_final_wf, data = pokemon_train)
```

```{r}
augment(rf_final_fit, new_data = pokemon_test) %>%
  roc_auc(truth = type_1, estimate = .pred_Bug:.pred_Water)
```
```{r}
# roc curves for the different factor levels
augment(rf_final_fit, new_data = pokemon_test) %>%
  roc_curve(truth = type_1, estimate = .pred_Bug:.pred_Water) %>%
  autoplot()
```

```{r}
# visualization of the accuracy
augment(rf_final_fit, new_data = pokemon_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type="heatmap")
```
My model was best at predicting Psychic type and worst at predicting Grass type.

## For 231 Students

### Exercise 11

Using the `abalone.txt` data from previous assignments, fit and tune a random forest model to predict `age`. Use stratified cross-validation and select ranges for `mtry`, `min_n`, and `trees`. Present your results. What was the model's RMSE on your testing set?

```{r}
abalone <- read_csv(file = "data/abalone.csv")
abalone <- abalone %>%
  mutate(age = rings+1.5)
```

```{r}
set.seed(42069) # for reproducibility

# get stratified train/testing sets
abalone_split <- initial_split(abalone, prop = 0.80,
                                strata = age)
abalone_train <- training(abalone_split)
abalone_test <- testing(abalone_split)

abalone_folds <- vfold_cv(abalone_train, v=5, strata=age)
```

```{r}
# define recipe
abalone_recipe <- recipe(age ~ ., data = abalone_train%>%select(-rings)) %>%
  step_dummy(all_nominal_predictors())%>%
  step_interact(terms = ~ starts_with("type"):shucked_weight + longest_shell:diameter + shucked_weight:shell_weight)%>%
  step_center(all_predictors())%>%
  step_scale(all_predictors())
```

```{r}
# set up model and workflow
rf_spec <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("regression")

rf_wf <- workflow() %>%
  add_model(rf_spec)%>%
  add_recipe(abalone_recipe)

# grid of parameter values to search over
param_grid <- grid_regular(mtry(range = c(1,8)),
                           trees(range = c(2,200)),
                           min_n(range = c(2,40)),
                           levels = 8)
```

```{r,eval=FALSE}
# get results of tuning grid
set.seed(42069)

ab_tune_res <- tune_grid(
  rf_wf, 
  resamples = abalone_folds, 
  grid = param_grid, 
  metrics = metric_set(rmse)
)
```
```{r,include=FALSE}
load("ab_tune_res.rda")
```

```{r}
# plot results
autoplot(ab_tune_res)
```

```{r}
# find the parameter values that yield the lowest rmse
rf_rmse <- collect_metrics(ab_tune_res)%>%
  arrange(mean)
rf_rmse%>%
  head()
```
We see that the model where `mtry` is 5, `trees` is 200, and `min_n` is 29 yields the lowest training rmse of 2.166103.

```{r}
best_params <- select_best(ab_tune_res)

rf_final <- finalize_workflow(rf_wf, best_params)

rf_final_fit <- fit(rf_final, data = abalone_train)

augment(rf_final_fit, new_data = abalone_test) %>%
  rmse(truth = age, estimate = .pred)
```
We see that the test rmse is 2.036142 which is actually lower than the training rmse.